{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this project, I analyze a dataset containing restaurant reviews and whether they are positive or negative. From this, I will conduct sentiment analysis to teach the program to predict whether a review is positive or negative. The program can often predict with 80% accuracy. \n",
    "\n",
    "#### This project is based on the following guide: https://towardsdatascience.com/a-beginners-guide-to-sentiment-analysis-in-python-95e354ea84f6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt, figure\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first thing we should analyze is the most common words used in these reviews. As we can see, the most common words in all reviews are 'good','place' and 'food'. In positive reviews, they are 'food', 'great', and 'good'. In negative reviews, they are 'back','place', and 'food'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the data frame\n",
    "reviews=pd.read_csv(os.path.join(os.getcwd(), \"Restaurant_Reviews.csv\"))\n",
    "stop = set(stopwords.words('english'))\n",
    "#Creating a figure with three subplots to display the most common words in positive, negative, and all reviews.\n",
    "fig, common_axs = plt.subplots(1, 3,figsize=(20,7.5)) \n",
    "fig.suptitle(\"The Top 50 Most Common Words\", fontsize=20)\n",
    "def common_bar(df,position,review_type): #position indicates which subplot the graph will be plotted on. \n",
    "    #The next 6 lines of code are from https://www.kaggle.com/edhirif/word-cloud-alternative-using-nltk\n",
    "    review_str = df[\"Review\"].str.cat(sep = ' ') #Takes each row of the reviews dataframe and puts it into a single string. Each review is separated with a space.\n",
    "    list_of_words = [i.lower() for i in wordpunct_tokenize(review_str) if i.lower() not in stop and i.isalpha()] #Lowercases each word in every review if it is not in the list of stop words and all the characters in the word are letters.\n",
    "    wordfreqdist = nltk.FreqDist(list_of_words)#Counts how frequent each word is.\n",
    "    mostcommon = wordfreqdist.most_common(25) #creates a list of the 25 most common words.\n",
    "    common_axs[position].barh(range(len(mostcommon)),[val[1] for val in mostcommon])\n",
    "    common_axs[position].set_title(review_type + ' Reviews')\n",
    "    plt.sca(common_axs[position])\n",
    "    plt.yticks(range(len(mostcommon)),[val[0] for val in mostcommon],fontsize=15)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(wspace=.30)\n",
    "common_bar(reviews,0,\"All\") \n",
    "common_bar(reviews[reviews[\"Liked\"]==1],1,\"Positive\")\n",
    "common_bar(reviews[reviews[\"Liked\"]==0],2,\"Negative\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next step is to clean the data by removing punctuation from the reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text): ##Replaces punctuation with an empty string.\n",
    "    final = \"\".join(char for char in text if char not in (\"?\", \".\", \";\", \":\",  \"!\",'\"')) #Joins together each character in the text if it is not punctuation. \n",
    "    return final\n",
    "reviews[\"Review\"]=reviews[\"Review\"].apply(remove_punctuation)\n",
    "reviews = reviews.dropna(subset=['Review'])\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on the next cell. \n",
    "    In the previous cell, there is a 2D array. Every row in the data frame is recorded as a separate list in the array. A data frame of 100 rows would have an array of 100 lists. Each list contains a series of numbers, one for each unique word found in the data frame. If there are 50 unique words in the data frame, then each series in each list of the array would have 50 entries. If the data frame has 100 rows and 50 unique words, then there would be 100 lists, and each list would have 50 entries. \n",
    "    For every entry in each list, there is a number representing how many times each of the unique words appears in the row corresponding to that list. If one of the unique words is 'hello' and the first row contains the following string: \"hi my name is michael\" then the entry for 'hello' in the first list would be 0 as 'hello' does not appear in the first row. The entries for 'hi' 'my', 'name', 'is', and 'michael' would all be 1's in the first list as each of them appear once.\n",
    "    If we wanted to know which entry corresponds with which number (which word is the 2nd entry of every list for example), we could simply print (vectorizer.get_feature_names()). This will return a list of words, and the index of each word in the list corresponds with the index of each entry in the array. I have printed the last 10 words in the list above. The last word is \"zero\" so the last entry of each list in the array will have a number denoting how many times \"zero\" appears in that row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(reviews[\"Review\"],reviews[\"Liked\"],test_size=.2) #Splitting the dataset into testing and training datasets.\n",
    "#X_train and X_test are the reviews; y_train and y_test are the numbers indicating the sentiment of each review. \n",
    "vectorizer = CountVectorizer(token_pattern=r'\\w+') #Creates a vectorizer object which will count the frequency of each word.\n",
    "train_matrix=vectorizer.fit_transform(X_train)\n",
    "test_matrix=vectorizer.transform(X_test)\n",
    "#The next three print statements are meant to help explain the note above.\n",
    "print (train_matrix.toarray())\n",
    "print (vectorizer.get_feature_names()[-10:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that the data is cleaned and formatted, teach the program to make predictions based on the reviews from the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_matrix,y_train)\n",
    "predictions = lr.predict(test_matrix)#This is an array of the predicted values for the testing datset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The program has made its predictions, so we can examine how well it did below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,predictions)\n",
    "#The code used to create the confusion matrix is from https://stackoverflow.com/questions/19233771/sklearn-plot-confusion-matrix-with-labels\n",
    "fig, ax= plt.subplots(figsize=(10,7.5))\n",
    "sns.set(font_scale=1.5) # Adjust to fit\n",
    "sns.heatmap(cm, cmap=\"coolwarm\",annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "ax.set_xlabel('Predicted Ssentiment', fontsize=15);ax.set_ylabel('Actual Sentiment',fontsize=15); \n",
    "ax.set_title('Confusion Matrix',fontsize=15); \n",
    "ax.xaxis.set_ticklabels(['Positive', 'Negative'],fontsize=15); ax.yaxis.set_ticklabels(['Positive', 'Negative'],fontsize=15);\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
